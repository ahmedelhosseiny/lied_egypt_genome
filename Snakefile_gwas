# kate: syntax python;
# Run rule list_diseases first!

import gzip
import os.path
from decimal import Decimal
import math


rule all:
    input: "gwas/results/num_diseases.txt", \
           expand("gwas/results/num_{analysis}_per_disease.txt", analysis= \
                 ["associations","europeanassociations","recurrentloci","loci"])


########### Preprocessing GWAS catalog data ###########

# Get the GWAS catalog
rule get_gwas_catalog:
    output: "gwas/results/alternative"
    shell: "wget --directory-prefix=gwas/results https://www.ebi.ac.uk/gwas/api/search/downloads/alternative 2> /dev/null"

# Get the GWAS catalog
rule get_gwas_catalog_ancestry:
    output: "gwas/results/ancestry"
    shell: "wget --directory-prefix=gwas/results https://www.ebi.ac.uk/gwas/api/search/downloads/ancestry 2> /dev/null"

rule list_diseases:
    input: "gwas/results/alternative"
    output: "gwas/results/custom_disease_ids.txt"
    run:
        disease_ids = {}
        with open(input[0],"r") as f_in, open(output[0],"w") as f_out:
            for line in f_in:
                # Write custom header
                if line[:4] == "DATE":
                    f_out.write("MAPPED_TRAIT\tMAPPED_TRAIT_URI\tCUSTOM_ID\n")
                    continue
                s = line.split("\t")
                new_disease_id = "-".join([x.split("/")[-1] for x in s[35].split(",")])
                if not new_disease_id in disease_ids:
                    f_out.write("\t".join([s[34],s[35],new_disease_id])+"\n")
                    disease_ids[new_disease_id] = True

rule num_diseases:
    input: "gwas/results/custom_disease_ids.txt"
    output: "gwas/results/num_diseases.txt"
    shell: "cat {input} | grep -v 'MAPPED_TRAIT' | wc -l > {output}"

GWAS_CATALOG_DISEASES = []
if os.path.exists("gwas/results/custom_disease_ids.txt"):
    with open("gwas/results/custom_disease_ids.txt","r") as f_in:
        for line in f_in:
            # Skip header and skip lines without mapped trait
            if line[:6] == "MAPPED" or line == "\t\t\n":
                continue
            GWAS_CATALOG_DISEASES.append(line.strip("\n").split("\t")[-1])
#    print(GWAS_CATALOG_DISEASES)

# Use the GWAS catalog data and make lists of variants for every disease / 
# phenotype using the ontology IDs provided in column MAPPED_TRAIT_URI as file 
# Watchout: there are 249 associations without mapped ID
rule make_disease_specific_gwas_lists:
    input: "gwas/results/alternative"
    output: expand("gwas/results/{disease}_catalog.tab", disease=GWAS_CATALOG_DISEASES)
    run:
        i = 0
        for filename in output:
            with open(input[0],"r") as f_in, open(filename,"w") as f_out:
                for line in f_in:
                    if line[:4] == "DATE":
                        f_out.write(line)
                        continue
                    s = line.split("\t")
                    disease_id = "-".join([x.split("/")[-1] for x in s[35].split(",")])
                    if disease_id == GWAS_CATALOG_DISEASES[i]:
                        f_out.write(line)                        
                i += 1 

rule num_associations_per_disease:
    input: expand("gwas/results/{disease}_catalog.tab", disease=GWAS_CATALOG_DISEASES)
    output: "gwas/results/num_associations_per_disease.txt"
    run:
        i = 0
        for filename in input:
            with open(input[i],"r") as f_in, open(output[0],"a") as f_out:
                f_out.write(GWAS_CATALOG_DISEASES[i]+"\t"+str(len(f_in.readlines())-1)+"\n")
            i += 1

# Make a list of disease-specific gwas hits only for studies using individuals
# of European ancestry
rule filter_disease_specific_gwas_lists_for_european:
    input: "gwas/results/{disease}_catalog.tab",
           "gwas/results/ancestry"
    output: "gwas/results/{disease}_european.tab"
    run:
        # Get study IDs of studies with only European individuals
        study_accessions = {}
        with open(input[1],"r") as f_in, open(output[0],"w") as f_out:
            for line in f_in:
                if line[:5] == "STUDY":
                    f_out.write(line)
                    continue
                s = line.split("\t")
                # We use the column "BROAD ANCESTRAL CATEGORY", and of this only
                # European only
                ancestry = s[8]
                if ancestry == "European":
                    study_accessions[s[0]] = True
        with open(input[0],"r") as f_in, open(output[0],"w") as f_out:
            for line in f_in:
                if line[:4] == "DATE":
                    f_out.write(line)
                    continue
                # get the study accession of this association              
                study_accession = line.split("\t")[36]
                if study_accession in study_accessions:
                    f_out.write(line)

rule num_european_associations_per_disease:
    input: expand("gwas/results/{disease}_european.tab", disease=GWAS_CATALOG_DISEASES)
    output: "gwas/results/num_europeanassociations_per_disease.txt"
    run:
        i = 0
        for filename in input:
            with open(input[i],"r") as f_in, open(output[0],"a") as f_out:
                f_out.write(GWAS_CATALOG_DISEASES[i]+"\t"+str(len(f_in.readlines())-1)+"\n")
            i += 1

rule filter_disease_specific_gwas_lists_for_european_all:
    input: expand("gwas/results/{disease}_european.tab", disease=GWAS_CATALOG_DISEASES)

# Filter disease-specific associations such that only associations within 1 MB
# of another association are kept
rule filter_disease_specific_gwas_lists_for_recurrent_loci:
    input: "gwas/results/{disease}_european.tab"
    output: "gwas/results/{disease}_recurrentloci.tab"
    run:
        # Record all positions for this disease
        positions = []
        with open(input[0],"r") as f_in:
            for line in f_in:
                if line[:4] == "DATE":
                    continue
                s = line.split("\t")
                chrom = s[11]
                # Disregard assoc. without one unique position specified
                if s[12] == "" or "x" in s[12] or ";" in s[12]:
                    continue
                pos = int(s[12])
                positions.append([chrom,pos,line])
        # Go again over the associations
        with open(input[0],"r") as f_in, open(output[0],"w") as f_out:
            for line in f_in:
                if line[:4] == "DATE":
                    f_out.write(line)
                    continue
                s = line.split("\t")
                chrom = s[11]
                # Disregard assoc. without position specified
                if s[12] == "" or "x" in s[12] or ";" in s[12]:
                    continue
                pos = int(s[12])
                for prev_chrom,prev_pos,prev_line in positions:
                    if prev_line == line or not chrom == prev_chrom:
                        continue
                    if abs(pos-prev_pos)<1000000:
                        f_out.write(line)
                        break

rule num_recurrentloci_per_disease:
    input: expand("gwas/results/{disease}_recurrentloci.tab", disease=GWAS_CATALOG_DISEASES)
    output: "gwas/results/num_recurrentloci_per_disease.txt"
    run:
        i = 0
        for filename in input:
            with open(input[i],"r") as f_in, open(output[0],"a") as f_out:
                f_out.write(GWAS_CATALOG_DISEASES[i]+"\t"+str(len(f_in.readlines())-1)+"\n")
            i += 1

rule filter_disease_specific_gwas_lists_for_recurrent_loci_all:
    input: expand("gwas/results/{disease}_recurrentloci.tab", disease=GWAS_CATALOG_DISEASES)

POPULATIONS_EUR = ["CEU","FIN","GBR","IBS","TSI"]
rule extract_european_1000g:
    input: "1000_genomes/integrated_call_samples_v2.20130502.ALL.ped"
    output: "gwas/ld/keep_indiv.txt"
    run:
        with open(input[0],"r") as f_in, open(output[0],"w") as f_out:
            for line in f_in:
                s = line.split("\t")
                if s[6] in POPULATIONS_EUR and s[13] == "1":
                    f_out.write(s[1]+"\n")

# Selecting from the VCF files those individuals that are to be used
# Keeping only bi-allelic variants (min-allele = max-allele = 2)
rule select_european_1000g_individual_genotypes:
    input: "1000_genomes/ALL.chr{chr}_GRCh38.genotypes.20170504.vcf.gz",
           "gwas/ld/keep_indiv.txt"
    output: "gwas/ld/EUR.chr{chr}_GRCh38.vcf.gz"
    params: log_base=lambda wildcards, output: output[0][:-7]
    conda: "envs/genotype_pcs.yaml"
    shell: "vcftools --gzvcf {input[0]} " + \
                    "--keep {input[1]} " + \
                    "--recode-INFO-all " + \
                    "--recode " + \
                    "--out {params.log_base} " + \
                    "--stdout | bgzip > {output[0]}"

# Concatenate the vcf file from several chromosomes
# --pad-missing: Write '.' in place of missing columns. Useful for joining chrY 
# with the rest.
rule concatenate_european_chr_vcfs:
    input: expand("gwas/ld/EUR.chr{chr}_GRCh38.vcf.gz", \
                   chr=[str(x) for x in range(1,23)])
    output: "gwas/ld/EUR_GRCh38.vcf.gz"
    conda: "envs/genotype_pcs.yaml"
    shell: "vcf-concat --pad-missing {input} | bgzip > {output}"

rule filter_european_snps:
    input: "gwas/ld/EUR_GRCh38.vcf.gz"
    output: "gwas/ld/EUR_GRCh38_common.vcf.gz"
    params: log_base=lambda wildcards, output: output[0][:-7]
    conda: "envs/genotype_pcs.yaml"
    shell: "vcftools --gzvcf {input[0]} " + \
                    "--min-alleles 2 " + \
                    "--max-alleles 2 " + \
                    "--maf 0.05 " + \
                    "--hwe 0.000001 " + \
                    "--recode-INFO-all " + \
                    "--recode " + \
                    "--out {params.log_base} " + \
                    "--stdout | bgzip > {output[0]}"

# Extract the rsids from GWAS of the specific disease
rule get_snp_ids:
    input: "gwas/results/{disease}_recurrentloci.tab"
    output: "gwas/ld/{disease}.rsids"
    run:
        with open(input[0],"r") as f_in, open(output[0],"w") as f_out:
            for line in f_in:
                if line[:4] == "DATE":
                    continue
                f_out.write(line.split("\t")[20].split("-")[0]+"\n")

rule european_vcf_for_disease:
    input: "gwas/ld/EUR_GRCh38_common.vcf.gz",
           "gwas/ld/{disease}.rsids"
    output: "gwas/ld/{disease}_EUR.vcf.gz"
    params: log_base=lambda wildcards, output: output[0][:-7]
    conda: "envs/genotype_pcs.yaml"
    shell: "vcftools --gzvcf {input[0]} " + \
                    "--snps {input[1]} " + \
                    "--recode-INFO-all " + \
                    "--recode " + \
                    "--out {params.log_base} " + \
                    "--stdout | bgzip > {output[0]}"

GWAS_CATALOG_DISEASES_LOCI = []
if os.path.exists("gwas/results/num_recurrentloci_per_disease.txt"):
    with open("gwas/results/num_recurrentloci_per_disease.txt","r") as f_in:
        for line in f_in:
            s = line.strip("\n").split("\t")
            if not s[1] == "0":
                GWAS_CATALOG_DISEASES_LOCI.append(s[0])

rule num_recurrentloci_in_vcf_per_disease:
    input: expand("gwas/ld/{disease}_EUR.vcf.gz", disease=GWAS_CATALOG_DISEASES_LOCI)
    output: "gwas/results/num_recurrentlocivcf_per_disease.txt"
    run:
        i = 0
        for filename in input:
            num_ass = 0
            with gzip.open(input[i],"r") as f_in, open(output[0],"a") as f_out:
                for line in f_in:
                    if line.decode()[0] == "#":
                        continue
                    num_ass += 1
                f_out.write(GWAS_CATALOG_DISEASES_LOCI[i]+"\t"+str(num_ass)+"\n")
            i += 1

GWAS_CATALOG_DISEASES_LOCI_VCF = []
if os.path.exists("gwas/results/num_recurrentlocivcf_per_disease.txt"):
    with open("gwas/results/num_recurrentlocivcf_per_disease.txt","r") as f_in:
        for line in f_in:
            s = line.strip("\n").split("\t")
            if not s[1] in ["0","1"]:
                GWAS_CATALOG_DISEASES_LOCI_VCF.append(s[0])

# Compute the LD between any pair of SNPs
# Output the LDs less than 0.8 
# Compute only for SNPs less than 1MB apart from each other
rule compute_ld_1000g:
    input: "gwas/ld/{disease}_EUR.vcf.gz",
    output: "gwas/ld/{disease}.geno.ld"
    params: log_base=lambda wildcards, output: output[0][:-8]
    conda: "envs/genotype_pcs.yaml"
    shell: "vcftools --gzvcf {input[0]} " + \
                    "--geno-r2 " + \
                    "--ld-window-bp 1000000 " + \
                    "--min-r2 0.8 " + \
                    "--out {params.log_base} "

rule num_recurrentloci_in_ld_per_disease:
    input: expand("gwas/ld/{disease}.geno.ld", disease=GWAS_CATALOG_DISEASES_LOCI_VCF)
    output: "gwas/results/num_recurrentlocild_per_disease.txt"
    run:
        i = 0
        for filename in input:
            num_ass = 0
            with open(input[i],"r") as f_in, open(output[0],"a") as f_out:
                for line in f_in:
                    if line[0] == "CHR":
                        continue
                    num_ass += 1
                f_out.write(GWAS_CATALOG_DISEASES_LOCI_VCF[i]+"\t"+str(num_ass)+"\n")
            i += 1

GWAS_CATALOG_DISEASES_LOCI_LD = []
if os.path.exists("gwas/results/num_recurrentlocild_per_disease.txt"):
    with open("gwas/results/num_recurrentlocild_per_disease.txt","r") as f_in:
        for line in f_in:
            s = line.strip("\n").split("\t")
            # There must be more than the header line, i.e. at least one pair
            # of SNPs to continue
            if not s[1] == "1":
                GWAS_CATALOG_DISEASES_LOCI_LD.append(s[0])

# The idea is to keep one association per loci, where we define a locus as
# a region of 1MB with a tag association (this means we miss independent
# association signals that are closer than 1 MB from each other)
# We automatically select the association based on the number of occurrences 
# per rsid position plus this positions must be in LD>0.8 with at least one
# other association
rule keep_one_variant_per_locus:
    input: "gwas/results/{disease}_recurrentloci.tab",
           "gwas/ld/{disease}.geno.ld"
    output: "gwas/results/{disease}_loci.tab"
    run: 
        # Read in the association positions with high ld >0.8
        position_count = {}
        position2rsid = {}
        with open(input[1],"r") as f_in:
            for line in f_in:
                if line[:3] == "CHR":
                    continue
                s = line.split("\t")
                chrom = s[0]
                pos1 = s[1]
                pos2 = s[2]
                position_count["\t".join([chrom,pos1])] = 0
                position_count["\t".join([chrom,pos2])] = 0
        with open(input[0],"r") as f_in:
            for line in f_in:
                if line[:4] == "DATE":
                    continue
                s = line.split("\t")
                chrom = s[11]
                pos = s[12]
                # Only positions occurring at least once in ld with another ass.
                if "\t".join([chrom,pos]) in position_count:
                    position_count["\t".join([chrom,pos])] += 1
        sorted_pc = sorted(position_count.items(), key = lambda x: -1*x[1])
        print(sorted_pc)
        loci = [sorted_pc[0][0].split("\t")]
        # Go over the positions in descending order of their occurrences
        for chr_pos,occ in sorted_pc:
            chrom,pos = chr_pos.split("\t")
            locus_new = True
            for loc_chrom,loc_pos in loci:
                if not chrom == loc_chrom:
                    continue
                if (int(pos)-int(loc_pos)) <= 1000000:
                    locus_new = False
            if locus_new:
                loci.append([chrom,pos])
        print(loci)
        # Go over the input gwas ass. file and write only those ass. to output
        # which are uniquely defining a locus
        with open(input[0],"r") as f_in, open(output[0],"w") as f_out:
            for line in f_in:
                if line[:4] == "DATE":
                    f_out.write(line)
                    continue
                s = line.split("\t")
                chrom = s[11]
                pos = s[12]
                if [chrom,pos] in loci:
                    f_out.write(line)

rule num_european_loci_per_disease:
    input: expand("gwas/results/{disease}_loci.tab", disease=GWAS_CATALOG_DISEASES_LOCI_LD)
    output: "gwas/results/num_loci_per_disease.txt"
    run:
        i = 0
        for filename in input:
            with open(input[i],"r") as f_in, open(output[0],"a") as f_out:
                f_out.write(GWAS_CATALOG_DISEASES_LOCI_LD[i]+"\t"+str(len(f_in.readlines())-1)+"\n")
            i += 1

# Make a long list for the tag snps obtained like this combining all diseases
rule tag_snp_list:
    input: expand("gwas/results/{disease}_loci.tab", disease=GWAS_CATALOG_DISEASES_LOCI_LD)    
    output: "gwas/tag_snps/tag_snps.txt"
    shell: "cat {input} | grep -v 'SNPS' | cut -f 12,13,22 | sort | uniq > {output}"

rule tag_rsids:
    input: "gwas/tag_snps/tag_snps.txt"
    output: "gwas/tag_snps/tag_snps.rsids"
    shell: "cat {input} | cut -f 3 > {output}"

# How much does allele frequency differ for the tag snps
# For this, first get allele frequency in European data 
rule select_tag_european:
    input: "gwas/ld/EUR_GRCh38_common.vcf.gz", "gwas/tag_snps/tag_snps.rsids"
    output: "gwas/tag_snps/tag_snps_EUR.vcf.gz"
    params: log_base=lambda wildcards, output: output[0][:-7]
    conda: "envs/genotype_pcs.yaml"
    shell: "vcftools --gzvcf {input[0]} " + \
                    "--snps {input[1]} " + \
                    "--recode-INFO-all " + \
                    "--recode " + \
                    "--out {params.log_base} " + \
                    "--stdout | bgzip > {output[0]}"

rule af_tag_european:
    input: "gwas/tag_snps/tag_snps_EUR.vcf.gz"
    output: "gwas/tag_snps/tag_snps_EUR.frq"
    params: log_base=lambda wildcards, output: output[0][:-4]
    conda: "envs/genotype_pcs.yaml"
    shell: "vcftools --gzvcf {input[0]} " + \
                    "--freq " + \
                    "--out {params.log_base} "

rule select_tag_egyptian:
    input: "vep_annotation/vep.vcf.gz", \
           "gwas/tag_snps/tag_snps.rsids"
    output: "gwas/tag_snps/tag_snps_EGP.vcf.gz"
    params: log_base=lambda wildcards, output: output[0][:-7]
    conda: "envs/genotype_pcs.yaml"
    shell: "vcftools --gzvcf {input[0]} " + \
                    "--snps {input[1]} " + \
                    "--recode-INFO-all " + \
                    "--recode " + \
                    "--out {params.log_base} " + \
                    "--stdout | bgzip > {output[0]}"

rule af_tag_egyptian:
    input: "gwas/tag_snps/tag_snps_EGP.vcf.gz"
    output: "gwas/tag_snps/tag_snps_EGP.frq"
    params: log_base=lambda wildcards, output: output[0][:-4]
    conda: "envs/genotype_pcs.yaml"
    shell: "vcftools --gzvcf {input[0]} " + \
                    "--freq " + \
                    "--out {params.log_base} "

rule compare_tag_af:
    input: "gwas/tag_snps/tag_snps_EUR.frq", "gwas/tag_snps/tag_snps_EGP.frq"
    output: "gwas/tag_snps/tag_snps_af.txt", \
            "gwas/tag_snps/tag_snps_af_multiallelic.txt" 
    run:
        header = "\t".join(["CHROM","POS","REF","ALT","EUR_N_CHR","EUR_ALT_AF","EGP_N_CHR","EGP_ALT_AF"])
        header_multi = "\t".join(["CHROM","POS","REF","ALT","EUR_N_CHR","EUR_ALT_AF"])
        eur_af = {}
        eur_af_present = {}
        with open(input[0],"r") as f_in:
            for line in f_in:
                # Skip header
                if line[:5] == "CHROM":
                    continue
                s = line.strip("\n").split("\t")
                chrom = s[0]
                pos = s[1]
                # Make sure all tag SNPs are in fact bi-allelic
                assert(s[2] == "2")
                ref,ref_af = s[4].split(":")
                alt,alt_af = s[5].split(":")
                # Make sure frequencies add up to 1
                assert(0.9999<=float(ref_af)+float(alt_af)<=1.0001)
                nchrom = s[3]
                loc = "\t".join([chrom,pos,ref,alt])
                eur_af[loc] = [nchrom,alt_af]
                eur_af_present[loc] = [nchrom,alt_af]
        with open(input[1],"r") as f_in, open(output[0],"w") as f_out, \
             open(output[1],"w") as f_out_multi:
            for line in f_in:
                # Skip header; write new header
                if line[:5] == "CHROM":
                    f_out.write(header+"\n")
                    f_out_multi.write(header_multi+"\n")
                    continue
                s = line.strip("\n").split("\t")
                chrom = s[0]
                if chrom[:3] == "chr":
                    chrom = chrom[3:]
                pos = s[1]
                # Make sure all tag SNPs are in fact bi-allelic
                assert(s[2] == "2")
                ref,ref_af = s[4].split(":")
                alt,alt_af = s[5].split(":")
                # Make sure frequencies add up to 1
                assert(0.9999<=float(ref_af)+float(alt_af)<=1.0001)
                nchrom = s[3]
                loc = "\t".join([chrom,pos,ref,alt])
                egp_af = [nchrom,alt_af]
                if not loc in eur_af:
                    f_out_multi.write(loc+"\t"+"\t".join(egp_af)+"\n")
                else:
                    f_out.write(loc+"\t"+"\t".join(eur_af[loc])+"\t"+"\t".join(egp_af)+"\n")
                    del eur_af_present[loc]
                # remove this locus, it is present in Egyptian data
            # Those tag SNPs not called in the Egyptian data have allele 
            # frequency of zero
            for loc in eur_af_present:
                 f_out.write(loc+"\t"+"\t".join(eur_af_present[loc])+"\t220\t0\n")

rule plot_tag_af_diff:
    input: "gwas/tag_snps/tag_snps_af.txt"
    output: "gwas/tag_snps/tag_af_diff_hist.pdf", \
            "gwas/tag_snps/tag_af_diff_scatter.pdf", \
            "gwas/tag_snps/tag_af_diff_missing.pdf"
    script: "scripts/plot_tag_af_diff.R"
        
            

########### Matching Egyptian common variants and GWAS data ###########

# Extract those Egyptian population-specific SNPs that are within 100kb of a SNP 
# listed in the GWAS catalog
rule egyptian_popspecific_in_proximity:
    input: "gwas/results/hg38_gwas_{disease}_catalog.tab",
           "vep_annotation/vep_egyptian_popspecific.vcf.gz"
    output: "gwas/results_{distance}/hg38_gwas_{disease}_{distance}_matched.txt"
    run:
        # Get the distance; SNPs within distance of the GWAS SNP are matched
        dist = int(wildcards.distance)
        # Read in positions of Egyptian common SNPs
        vcf = {}
        with gzip.open(input[1], 'rb') as f_vcf:
            for line in f_vcf:
                decoded_line = line.decode()
                if decoded_line[0] == '#':
                    header_vcf = decoded_line[1:]
                    continue
                s = decoded_line.split("\t")
                chrom = s[0]
                if chrom[:3] == "chr":
                    chrom = chrom[3:]
                pos = s[1]
                if chrom in vcf:
                    vcf[chrom].append([chrom,int(pos),decoded_line])
                else:
                    vcf[chrom] = [[chrom,int(pos),decoded_line]]
        # Go over the GWAS catalog lines associations for this disease
        with open(input[0],"r") as f_in, open(output[0],"w") as f_out:
            for line in f_in:
                # Skip header
                if line[:4] == "DATE":
                    f_out.write(line.strip("\n")+"\t"+header_vcf)
                    continue
                s = line.split("\t")
                chrom = s[11]
                # Skip GWAS catalog associations without position specified
                if s[12] == "" or " x " in s[12] or ";" in s[12]:
                    continue
                pos = int(s[12])
                for variant in vcf[chrom]:
                    if pos-dist <= variant[1] <= pos+dist:
                        f_out.write(line.strip("\n")+"\t"+variant[2]) 

rule egyptian_popspecific_in_proximity_all:
    input: expand("gwas/results_{distance}/hg38_gwas_{disease}_{distance}_matched.txt", \
                  disease=GWAS_CATALOG_DISEASES, \
                  distance=["0","10","100","1000","10000","100000"])
        

rule generate_paper_numbers:
    input: "gwas/results_table/number_of_index_snps.txt"

